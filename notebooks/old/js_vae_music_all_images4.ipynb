{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "138d15a5",
   "metadata": {
    "heading_collapsed": true,
    "id": "138d15a5"
   },
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8092c295",
   "metadata": {
    "hidden": true,
    "id": "8092c295"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 16:55:21.519330: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 16:55:22.043555: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-14 16:55:22.151417: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-14 16:55:22.151441: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-14 16:55:22.245092: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-14 16:55:24.001799: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 16:55:24.002013: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-14 16:55:24.002036: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, LeakyReLU\n",
    "from tensorflow.keras.layers import Reshape, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow import keras\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import moviepy.editor as mp\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65605540-d827-42c1-9c90-e664887f6930",
   "metadata": {
    "hidden": true,
    "id": "65605540-d827-42c1-9c90-e664887f6930"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8bc88fa-4c37-46ea-b2a7-ded4ce39c3d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "f8bc88fa-4c37-46ea-b2a7-ded4ce39c3d5",
    "outputId": "ff945eb3-89c5-46dd-8679-c99c54b04bf8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "              tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "936fb478",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "936fb478",
    "outputId": "d325ab5f-9432-48d8-ca6d-f9b3af52270a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 253 files belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 15:38:28.490211: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "ds = image_dataset_from_directory('zelle', label_mode=None, image_size=(448, 448), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba304f6a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "foldername = 'zelle'\n",
    "\n",
    "def load_images_from_folder(foldername, output_folder):\n",
    "    latent_space = []\n",
    "    for filename in os.listdir(foldername):\n",
    "        with Image.open(os.path.join(foldername,filename)).resize((448,448)) as img:\n",
    "            if img is not None:\n",
    "                img = np.asarray(img)\n",
    "                encoded = vae.encoder(img.reshape(-1,448,448,3)/255)\n",
    "                latent_space.append(encoded[2])\n",
    "    return latent_space\n",
    "           \n",
    "latent_space = load_images_from_folder(foldername, \"encoded_images\")\n",
    "\n",
    "# images/255\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22467885-ddc1-49d1-8bf8-0494cb98f311",
   "metadata": {
    "hidden": true,
    "id": "22467885-ddc1-49d1-8bf8-0494cb98f311"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d62639e5-5a98-4461-98a0-7a5e9e8f90f1",
   "metadata": {
    "hidden": true,
    "id": "d62639e5-5a98-4461-98a0-7a5e9e8f90f1"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers import Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e6e6b96-c503-49b4-9d6a-9984c9034f4c",
   "metadata": {
    "hidden": true,
    "id": "4e6e6b96-c503-49b4-9d6a-9984c9034f4c"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1e364c4f-ba95-471b-8db3-c4d951cef0d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "1e364c4f-ba95-471b-8db3-c4d951cef0d2",
    "outputId": "be353915-8cf7-4655-9741-421350675c2f"
   },
   "outputs": [],
   "source": [
    "latent_dim = 200\n",
    "\n",
    "\n",
    "input_image = Input(shape=(448, 448, 3))\n",
    "    \n",
    "x = Conv2D(32, (3, 3), padding='same', activation=\"relu\")(input_image)\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "#x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "encoder_output = Dense(latent_dim, activation=\"relu\")(x)\n",
    "\n",
    "z_mean = tf.keras.layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = tf.keras.layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = tf.keras.Model(input_image, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "#encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "71f64634-286c-4c64-b363-2814a2006808",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "71f64634-286c-4c64-b363-2814a2006808",
    "outputId": "12edb207-c475-4a85-e0f7-1ce1cd7aaf64"
   },
   "outputs": [],
   "source": [
    "latent_inputs = tf.keras.Input(shape=(latent_dim,))  \n",
    "x = Dense(7*7*64, activation='tanh')(latent_inputs)\n",
    "x = Reshape((7, 7, 64))(x)\n",
    "x = Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(64, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(64, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(32, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(32, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(16, (3, 3), strides=2, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(16, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(16, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(8, (3, 3), strides=2, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(8, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "x = Conv2DTranspose(8, (3, 3), strides=1, padding='same', activation=\"relu\")(x)\n",
    "\n",
    "\n",
    "    \n",
    "decoder_output = Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "decoder = Model(inputs=latent_inputs, outputs=decoder_output, name=\"decoder\")\n",
    "#decoder.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c0e04a5-d383-41f2-92e3-59fc677a3d2e",
   "metadata": {
    "hidden": true,
    "id": "0c0e04a5-d383-41f2-92e3-59fc677a3d2e"
   },
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ab0e937-58c7-4840-b093-4fa6efb8f8c1",
   "metadata": {
    "hidden": true,
    "id": "9ab0e937-58c7-4840-b093-4fa6efb8f8c1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CallbackSaveModel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 10 == 0:\n",
    "            self.model.save_weights(f'/home/jupyter/model_{epoch}')\n",
    "\n",
    "save_model = CallbackSaveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d8651e2-b7e4-4c9e-823d-7ba8150a236b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "7d8651e2-b7e4-4c9e-823d-7ba8150a236b",
    "outputId": "72edfedb-e0ab-42bc-f668-3c7aa6317113",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41beecd7-f384-482c-af0d-b42d25453591",
   "metadata": {
    "hidden": true,
    "id": "41beecd7-f384-482c-af0d-b42d25453591",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fdf3939c1f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_weights('model_vae_2500') #load_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6219f6-9275-48fe-8bfc-f888bbd88c1d",
   "metadata": {
    "heading_collapsed": true,
    "id": "1b6219f6-9275-48fe-8bfc-f888bbd88c1d"
   },
   "source": [
    "# Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18979cab",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "loaded_images = load_images_from_folder(foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b0eb0b3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# norm_images = []\n",
    "# for i in loaded_images:\n",
    "#     norm_images.append(i/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a13b7298",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# #all images in latent space (i.e encoded all images)  \n",
    "# encoded_images = []\n",
    "# for image in norm_images:\n",
    "    \n",
    "#     encoded_images.append(vae.encoder(image.reshape(-1,448,448,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02a51369",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# decoded_images = []\n",
    "# for image in encoded_images:\n",
    "#     decoded_images.append(vae.decoder(image[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e5f405-e81c-4f03-84a9-204a73d25e9d",
   "metadata": {
    "id": "67e5f405-e81c-4f03-84a9-204a73d25e9d"
   },
   "source": [
    "# VAE reconstructed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "402ff2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hop_length = 512\n",
    "fps = 24\n",
    "\n",
    "\n",
    "import librosa\n",
    "import math\n",
    "\n",
    "y, sr = librosa.load('Drums.wav', sr=22050)\n",
    "\n",
    "audio_duration = librosa.get_duration(y=y, sr=sr)\n",
    "\n",
    "total_frames_float = audio_duration * fps\n",
    "total_frames = math.ceil(total_frames_float) \n",
    "\n",
    "sample_rate = round(total_frames_float / audio_duration * hop_length)\n",
    "\n",
    "y, sr = librosa.load('Drums.wav', sr=sample_rate)\n",
    "\n",
    "audio_duration = librosa.get_duration(y=y, sr=sr)\n",
    "onset_strengths = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "onset_times = librosa.times_like(onset_strengths, sr=sr)\n",
    "# calculate desired hop length based on 24 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "13a411b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.347900390625"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9487f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# onset_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "34539dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 / fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6dee788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate optimal n_fft length based on hop length\n",
    "n_fft = 2 ** (hop_length - 1).bit_length()\n",
    "\n",
    "# ENVELOPES WITH HOP LENGTH DETERMINED BY FPS\n",
    "onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length = hop_length, n_fft=n_fft)\n",
    "\n",
    "onset_env_norm = librosa.util.normalize(onset_env)\n",
    "\n",
    "onset_frames = librosa.onset.onset_detect(y=y, sr=sr, units='time')\n",
    "len(onset_frames)\n",
    "\n",
    "N_STEPS = len(onset_env)\n",
    "\n",
    "frame_duration = 1/fps\n",
    "audio_movement = []\n",
    "\n",
    "times = librosa.times_like(onset_env, sr=24)\n",
    "\n",
    "IMAGE_TIME_GAP = 5 # seconds, play with this\n",
    "IMAGE_FRAME_GAP = 5 * fps\n",
    "N_IMAGES = 10\n",
    "N_STEPS_IMAGES = N_STEPS / N_IMAGES \n",
    "\n",
    "# random_start_point = np.random.randint(0, len(encoded_images), size=(1))[0]\n",
    "# random_start_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "911cadcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.1"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_STEPS_IMAGES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d63df1",
   "metadata": {},
   "source": [
    "# Audio and video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "SBWrnKopYCDr",
   "metadata": {
    "id": "SBWrnKopYCDr"
   },
   "outputs": [],
   "source": [
    "def get_audio_params(audio_path, frame_rate=fps, hop_window=hop_length):\n",
    "    # Load song with 22050 sample rate\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    audio_duration = librosa.get_duration(y, sr=sr)\n",
    "    \n",
    "    total_frames_float = audio_duration * frame_rate\n",
    "    total_frames = math.ceil(total_frames_float)\n",
    "    \n",
    "    sample_rate = round(total_frames_float / audio_duration * hop_window)\n",
    "    \n",
    "    return total_frames, sample_rate, audio_duration\n",
    "\n",
    "def get_onset_info(audio_path, sample_rate, frame_rate=fps, hop_window=hop_length):\n",
    "    # Load\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    \n",
    "    # Onset strengths and normalize\n",
    "    onset_strengths = librosa.onset.onset_strength(y=y, sr=sample_rate, aggregate=np.median)\n",
    "    onset_strengths = librosa.util.normalize(onset_strengths)\n",
    "    \n",
    "    # Onset timestamps and frames\n",
    "    onset_times = librosa.times_like(onset_strengths, sr=sample_rate)\n",
    "    onset_frames = onset_times * frame_rate\n",
    "    \n",
    "    onset_info = np.concatenate([\n",
    "        onset_frames.reshape((-1, 1)),\n",
    "        onset_times.reshape((-1, 1)),\n",
    "        onset_strengths.reshape((-1, 1))\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Beat times\n",
    "    beat_times = librosa.beat.beat_track(y=y, sr=sample_rate, units='time')[1]\n",
    "    \n",
    "    return y, onset_info, beat_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14b24b80",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_duration() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m total_frames, sample_rate, audio_duration \u001b[38;5;241m=\u001b[39m \u001b[43mget_audio_params\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDrums.wav\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhop_window\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhop_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m total_frames, sample_rate, audio_duration\n",
      "Cell \u001b[0;32mIn [54], line 4\u001b[0m, in \u001b[0;36mget_audio_params\u001b[0;34m(audio_path, frame_rate, hop_window)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_audio_params\u001b[39m(audio_path, frame_rate\u001b[38;5;241m=\u001b[39mfps, hop_window\u001b[38;5;241m=\u001b[39mhop_length):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Load song with 22050 sample rate\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     y, sr \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mload(audio_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m22050\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     audio_duration \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_duration\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     total_frames_float \u001b[38;5;241m=\u001b[39m audio_duration \u001b[38;5;241m*\u001b[39m frame_rate\n\u001b[1;32m      7\u001b[0m     total_frames \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(total_frames_float)\n",
      "\u001b[0;31mTypeError\u001b[0m: get_duration() takes 0 positional arguments but 1 positional argument (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "total_frames, sample_rate, audio_duration = get_audio_params('Drums.wav', frame_rate=fps, hop_window=hop_length)\n",
    "total_frames, sample_rate, audio_duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb64f75",
   "metadata": {},
   "source": [
    "# Latent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1e7ddfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(librosa.times_like(onset_env, sr=sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "772a5731",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [121], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;66;03m#print(onset_env[i])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         inter_images_decoded = vae.decoder(interpolation_array)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         inter_images = (inter_images_decoded.numpy() * 255).astype('uint8')\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(interpolation_images)\n\u001b[0;32m---> 13\u001b[0m PASS_THIS_TO_THE_VIDEO_GENERATING_FUNCTION \u001b[38;5;241m=\u001b[39m \u001b[43mframes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [121], line 6\u001b[0m, in \u001b[0;36mframes\u001b[0;34m(latent_spaces)\u001b[0m\n\u001b[1;32m      4\u001b[0m         start_lat_vec, end_lat_vec \u001b[38;5;241m=\u001b[39m latent_space[j], latent_space[j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m         step_vec \u001b[38;5;241m=\u001b[39m (end_lat_vec \u001b[38;5;241m-\u001b[39m start_lat_vec ) \u001b[38;5;241m/\u001b[39m N_STEPS_IMAGES \n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_STEPS_IMAGES\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      7\u001b[0m             interpolation_images\u001b[38;5;241m.\u001b[39mappend(start_lat_vec \u001b[38;5;241m+\u001b[39m (i \u001b[38;5;241m*\u001b[39m (step_vec \u001b[38;5;241m*\u001b[39m(onset_env_norm[i] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2000\u001b[39m))))\n\u001b[1;32m      8\u001b[0m             \u001b[38;5;66;03m#print(onset_env[i])\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#         inter_images_decoded = vae.decoder(interpolation_array)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         inter_images = (inter_images_decoded.numpy() * 255).astype('uint8')\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "def frames(latent_spaces):\n",
    "    interpolation_images = []\n",
    "    for j in range(N_IMAGES):\n",
    "        start_lat_vec, end_lat_vec = latent_space[j], latent_space[j+1]\n",
    "        step_vec = (end_lat_vec - start_lat_vec ) / N_STEPS_IMAGES \n",
    "        for i in range(0, N_STEPS_IMAGES):\n",
    "            interpolation_images.append(start_lat_vec + (i * (step_vec *(onset_env_norm[i] + 1 * 2000))))\n",
    "            #print(onset_env[i])\n",
    "#         inter_images_decoded = vae.decoder(interpolation_array)\n",
    "#         inter_images = (inter_images_decoded.numpy() * 255).astype('uint8')\n",
    "    return np.array(interpolation_images)\n",
    "\n",
    "PASS_THIS_TO_THE_VIDEO_GENERATING_FUNCTION = frames(latent_space)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a196290e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbad4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    total frames / fixed frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "96df73d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195, 1, 200)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PASS_THIS_TO_THE_VIDEO_GENERATING_FUNCTION.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fa23d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_duration = 1/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "606f18ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video output_video.mp4.\n",
      "MoviePy - Writing audio in output_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video output_video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "create_video_with_audio(PASS_THIS_TO_THE_VIDEO_GENERATING_FUNCTION, audio_file_path=\"Drums.wav\", output_video_path=\"./\", frame_duration=frame_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785F48yli9A_",
   "metadata": {
    "id": "785F48yli9A_",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(inter_images.shape[0]):\n",
    "    im = Image.fromarray(inter_images[i])\n",
    "    im.save(f'./images/image_{i}.jpg')\n",
    "\n",
    "# Load the audio file\n",
    "audio_clip = mp.AudioFileClip('Piano.wav')\n",
    "\n",
    "# Define the frame folder where the images are stored\n",
    "frame_folder = './images'\n",
    "\n",
    "# Get the list of image paths\n",
    "im_paths = [os.path.join(frame_folder, f'image_{i}.jpg') for i in range(N_STEPS)]\n",
    "\n",
    "# Load the images as frames\n",
    "frames = [mp.ImageClip(image_path).set_duration(frame_duration) # equal to 24fps\n",
    "          for image_path in im_paths]\n",
    "\n",
    "# Combine the frames into a video\n",
    "video_clip = mp.concatenate_videoclips(frames, method='chain')  # experiment with different method values\n",
    "\n",
    "# Overlay the audio on the video\n",
    "final_clip = video_clip.set_audio(audio_clip)\n",
    "\n",
    "# Write the final video to file\n",
    "final_clip.write_videofile(\"Aoutput_video.mp4\", fps=24) #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943bcc2a",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc71409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "from PIL import Image\n",
    "import moviepy.editor as mp\n",
    "\n",
    "def generate_latents(path_to_audio: str, vae, fps: int = 24, frame_folder: str = './images') -> None:\n",
    "    \"\"\"\n",
    "    Generates a video with frames interpolated between two latent vectors of a VAE, based on the onset\n",
    "    strength of an audio file.\n",
    "\n",
    "    Args:\n",
    "        path_to_audio (str): The path to the audio file\n",
    "        vae: The VAE model used for image generation\n",
    "        fps (int): The frames per second for the output video (default 24)\n",
    "        frame_folder (str): The folder path to save the frames (default './images')\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # load audio and sample rate\n",
    "    y, sr = librosa.load(path_to_audio)\n",
    "\n",
    "    # calculate desired hop length based on 24 fps\n",
    "    hop_length = int(sr / fps)\n",
    "\n",
    "    # calculate optimal n_fft length based on hop length\n",
    "    n_fft = 2 ** (hop_length - 1).bit_length()\n",
    "\n",
    "    # calculate onset envelopes and normalize \n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "    onset_env_norm = librosa.util.normalize(onset_env)\n",
    "\n",
    "    # get number of steps (frames)\n",
    "    n_steps = len(onset_env)\n",
    "\n",
    "    # generate encoded images for interpolation\n",
    "    #def create_step_vecs(N_IMAGES, IMAGE_FRAME_GAP, encoded_images):\n",
    "    step_vecs = []\n",
    "    for i in range(2):\n",
    "        step_vecs.append((encoded_images[2][i] - encoded_images[2][i+1]) / n_steps)\n",
    "\n",
    "    # interpolate between start and end latent vectors based on onset strength\n",
    "    interpolation_images = []\n",
    "    for i in range(n_steps):\n",
    "        interpolation_images.append(start_lat_vec + (i * (step_vec * (onset_env_norm[i] + 1))))\n",
    "    interpolation_array = np.array(interpolation_images)\n",
    "\n",
    "    # decode interpolated images\n",
    "    inter_images_decoded = vae.decoder(interpolation_array)\n",
    "    inter_images = (inter_images_decoded.numpy() * 255).astype('uint8')\n",
    "    \n",
    "\n",
    "    # save images to frame folder\n",
    "    for i in range(inter_images.shape[0]):\n",
    "        im = Image.fromarray(inter_images[i])\n",
    "        im.save(os.path.join(frame_folder, f'image_{i}.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_audio = \"Drums.wav\"\n",
    "\n",
    "frame_folder = './images'\n",
    "\n",
    "generate_latents(path_to_audio=path_to_audio, vae=vae, fps=fps, frame_folder=frame_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "034e473e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041666666666666664"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5dc13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "46f1a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from typing import List\n",
    "# import numpy as np\n",
    "# import moviepy.editor as mp\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "def create_video_with_audio(inter_images: np.ndarray, audio_file_path: str, output_video_path: str,\n",
    "                            frame_folder: str = './images', fps: int = 24, frame_duration: float = 1.0) -> None:\n",
    "    \"\"\"\n",
    "    Creates a video clip by concatenating a series of images with an audio clip.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    inter_images: numpy.ndarray\n",
    "        Array of images to be used as frames in the video clip.\n",
    "    audio_file_path: str\n",
    "        Path to the audio file to be overlaid on the video clip.\n",
    "    output_video_path: str\n",
    "        Path to save the output video file.\n",
    "    frame_folder: str, optional (default='./images')\n",
    "        Folder where the image frames are stored.\n",
    "    fps: int, optional (default=24)\n",
    "        Frames per second to use when creating the video clip.\n",
    "    frame_duration: float, optional (default=1.0)\n",
    "        Duration of each frame in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate inputs\n",
    "    if not isinstance(inter_images, np.ndarray):\n",
    "        raise TypeError(\"Input argument 'inter_images' must be a numpy.ndarray.\")\n",
    "    \n",
    "    if not isinstance(audio_file_path, str):\n",
    "        raise TypeError(\"Input argument 'audio_file_path' must be a string.\")\n",
    "    \n",
    "    if not isinstance(output_video_path, str):\n",
    "        raise TypeError(\"Input argument 'output_video_path' must be a string.\")\n",
    "    \n",
    "    if not isinstance(frame_folder, str):\n",
    "        raise TypeError(\"Input argument 'frame_folder' must be a string.\")\n",
    "    \n",
    "    if not isinstance(fps, int):\n",
    "        raise TypeError(\"Input argument 'fps' must be an integer.\")\n",
    "    \n",
    "    if not isinstance(frame_duration, (int, float)):\n",
    "        raise TypeError(\"Input argument 'frame_duration' must be a numeric value.\")\n",
    "    \n",
    "    if not os.path.exists(frame_folder):\n",
    "        raise ValueError(f\"Frame folder '{frame_folder}' does not exist.\")\n",
    "    \n",
    "    if not os.path.exists(audio_file_path):\n",
    "        raise ValueError(f\"Audio file '{audio_file_path}' does not exist.\")\n",
    "    \n",
    "    # save images to frame folder\n",
    "    for i in range(inter_images.shape[0]):\n",
    "        decoded = (vae.decoder(inter_images[i]).numpy().reshape(448,448,3) * 255).astype(np.uint8)\n",
    "        im = Image.fromarray(decoded)\n",
    "        im.save(os.path.join(frame_folder, f'image_{i}.jpg'))\n",
    "    \n",
    "    # Create frames from the input images\n",
    "    im_paths = [os.path.join(frame_folder, f'image_{i}.jpg') for i in range(inter_images.shape[0])]\n",
    "    frames = [mp.ImageClip(image_path).set_duration(frame_duration) for image_path in im_paths]\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio_clip = mp.AudioFileClip(audio_file_path)\n",
    "    \n",
    "    # Combine the frames into a video clip\n",
    "    video_clip = mp.concatenate_videoclips(frames, method='chain')\n",
    "    \n",
    "    # Overlay the audio on the video clip\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video clip to file\n",
    "    final_clip.write_videofile(\"output_video.mp4\", fps=fps, codec='libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onset_features(onset_info, beat_times, fixed_decay_frames=20, exp_decay_rate=0.2, decay_magnification=False):\n",
    "    \n",
    "    # Create column of zeroes as default value for both Linear and Exp Decay\n",
    "    onset_info = np.concatenate([onset_info, np.zeros((onset_info.shape[0], 1)), np.zeros((onset_info.shape[0], 1))], axis=1)\n",
    "    \n",
    "    # for each row\n",
    "    for i in range(onset_info.shape[0]):\n",
    "        # Skip the first value, makes life easy\n",
    "        if i == 0:\n",
    "            onset_info[i, 3] = 0\n",
    "        \n",
    "        \n",
    "        ## LINEAR\n",
    "        # If the timestamp is in beat_times, it's a peak\n",
    "        if onset_info[i, 1] in beat_times:\n",
    "            onset_info[i, 3] = onset_info[i, 2] # Linear Column\n",
    "            \n",
    "            # Decay Params\n",
    "            decay_factor = 1\n",
    "            if decay_magnification:\n",
    "                decay_factor *= (onset_info[i, 2] + 1)\n",
    "            \n",
    "            decay_frames = fixed_decay_frames * decay_factor\n",
    "            lin_decay_val = onset_info[i, 2] / decay_frames\n",
    "        \n",
    "        # Check if the previous value is zero or less than the decay_val -> 0\n",
    "        if onset_info[i - 1, 3] == 0. or abs(onset_info[i -1, 3]) < lin_decay_val:\n",
    "            pass\n",
    "        \n",
    "        # If previous value > 0, needs decay\n",
    "        elif onset_info[i - 1, 3] > 0: \n",
    "            onset_info[i, 3] = onset_info[i - 1, 3] - lin_decay_val\n",
    "        \n",
    "        \n",
    "        # EXPONENTIAL\n",
    "        # If the timestamp is in beat_times, it's a peak\n",
    "        if onset_info[i, 1] in beat_times:\n",
    "            onset_info[i, 4] = onset_info[i, 2] # Exp Column\n",
    "        \n",
    "        # Set current to zero if previous is zero or small number\n",
    "        elif onset_info[i - 1, 4] == 0 or onset_info[i - 1, 4] < 0.005: \n",
    "            pass\n",
    "        \n",
    "        # If previous value > 0, needs decay\n",
    "        elif onset_info[i - 1, 4] != 0: \n",
    "                onset_info[i, 4] = onset_info[i - 1, 4] * (1 - exp_decay_rate)\n",
    "    \n",
    "    return onset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7349e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_onset_info(audio_path, sample_rate, frame_rate=fps, hop_window=hop_length):\n",
    "    # Load\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    \n",
    "    # Onset strengths and normalize\n",
    "    onset_strengths = librosa.onset.onset_strength(y=y, sr=sample_rate, aggregate=np.median)\n",
    "    onset_strengths = librosa.util.normalize(onset_strengths)\n",
    "    \n",
    "    # Onset timestamps and frames\n",
    "    onset_times = librosa.times_like(onset_strengths, sr=sample_rate)\n",
    "    onset_frames = onset_times * frame_rate\n",
    "    \n",
    "    onset_info = np.concatenate([\n",
    "        onset_frames.reshape((-1, 1)),\n",
    "        onset_times.reshape((-1, 1)),\n",
    "        onset_strengths.reshape((-1, 1))\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Beat times\n",
    "    beat_times = librosa.beat.beat_track(y=y, sr=sample_rate, units='time')[1]\n",
    "    \n",
    "    return y, onset_info, beat_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f3ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, onset_info, beat_times = get_onset_info('Drums.wav', sample_rate=sr, frame_rate=fps, hop_window=hop_length)\n",
    "\n",
    "onset_info = create_onset_features(onset_info, beat_times, fixed_decay_frames=20, exp_decay_rate=0.10, decay_magnification=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05a6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "generate_video(path_to_audio='Drums.wav', vae=vae, fps=24, frame_folder='./images')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687816e6",
   "metadata": {},
   "source": [
    "# All audio/video functionality in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b83f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both functions in one \n",
    "\n",
    "\n",
    "# from typing import Tuple\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# import os\n",
    "# from PIL import Image\n",
    "# import moviepy.editor as mp\n",
    "\n",
    "def generate_video(path_to_audio: str, vae, fps: int = 24, frame_folder: str = './images') -> None:\n",
    "    \"\"\"\n",
    "    Generates a video with frames interpolated between two latent vectors of a VAE, based on the onset\n",
    "    strength of an audio file.\n",
    "\n",
    "    Args:\n",
    "        path_to_audio (str): The path to the audio file\n",
    "        vae: The VAE model used for image generation\n",
    "        fps (int): The frames per second for the output video (default 24)\n",
    "        frame_folder (str): The folder path to save the frames (default './images')\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # load audio and sample rate\n",
    "    y, sr = librosa.load(path_to_audio)\n",
    "\n",
    "    # calculate desired hop length based on 24 fps\n",
    "    hop_length = int(sr / fps)\n",
    "\n",
    "    # calculate optimal n_fft length based on hop length\n",
    "    n_fft = 2 ** (hop_length - 1).bit_length()\n",
    "\n",
    "    # calculate onset envelopes and normalize \n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr, hop_length=hop_length, n_fft=n_fft)\n",
    "    onset_env_norm = librosa.util.normalize(onset_env)\n",
    "\n",
    "    # get number of steps (frames)\n",
    "    n_steps = len(onset_env)\n",
    "\n",
    "    # generate encoded images for interpolation\n",
    "    encoded_images = vae.encoder(imgs[10:12].reshape(-1, 448, 448, 3))\n",
    "    start_lat_vec, end_lat_vec = encoded_images[0][0], encoded_images[0][1]\n",
    "    step_vec = (end_lat_vec - start_lat_vec) / n_steps\n",
    "\n",
    "    # interpolate between start and end latent vectors based on onset strength\n",
    "    interpolation_images = []\n",
    "    for i in range(n_steps):\n",
    "        interpolation_images.append(start_lat_vec + (i * (step_vec * (onset_env_norm[i] + 1))))\n",
    "    interpolation_array = np.array(interpolation_images)\n",
    "\n",
    "    # decode interpolated images\n",
    "    inter_images_decoded = vae.decoder(interpolation_array)\n",
    "    inter_images = (inter_images_decoded.numpy() * 255).astype('uint8')\n",
    "\n",
    "    # save images to frame folder\n",
    "    for i in range(inter_images.shape[0]):\n",
    "        im = Image.fromarray(inter_images[i])\n",
    "        im.save(os.path.join(frame_folder, f'image_{i}.jpg'))\n",
    "\n",
    "    # load the audio clip\n",
    "    audio_clip = mp.AudioFileClip(path_to_audio)\n",
    "\n",
    "    # get the list of image paths\n",
    "    im_paths = [os.path.join(frame_folder, f'image_{i}.jpg') for i in range(n_steps)]\n",
    "\n",
    "    # load the images as frames\n",
    "    frames = [mp.ImageClip(image_path).set_duration(1/fps) for image_path in im_paths]\n",
    "\n",
    "    # combine the frames into a video\n",
    "    video_clip = mp.concatenate_videoclips(frames, method='chain')\n",
    "\n",
    "    # overlay the audio on the video\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "\n",
    "    # write the final video to file\n",
    "    final_clip.write_videofile(\"output_video.mp4\", fps=fps, codec='libx264')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521f8ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb26bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m103"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
