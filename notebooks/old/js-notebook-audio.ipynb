{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import wavfile\n",
    "# samplerate, data = wavfile.read('Audio_file.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.        ]\n",
      " [ 0.67865563  0.85051624]\n",
      " [ 0.88230713  0.88470914]\n",
      " ...\n",
      " [-0.71618973 -0.26197822]\n",
      " [ 0.9901807   0.22072984]\n",
      " [ 0.59898683  0.99897792]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random \n",
    "\n",
    "sin_data = np.sin(data)\n",
    "\n",
    "print (sin_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44100\n"
     ]
    }
   ],
   "source": [
    "print (samplerate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy, scipy, matplotlib.pyplot as plt, IPython.display as ipd\n",
    "import librosa, librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load('Piano.wav') # D# piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050\n"
     ]
    }
   ],
   "source": [
    "print (sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmin = librosa.midi_to_hz(36)\n",
    "hop_length = 512\n",
    "C = librosa.cqt(x, sr=sr, fmin=fmin, n_bins=72, hop_length=hop_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logC = librosa.amplitude_to_db(numpy.abs(C))\n",
    "# plt.figure(figsize=(15, 5))\n",
    "# librosa.display.specshow(logC, sr=sr, x_axis='time', y_axis='cqt_note', fmin=fmin, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent_vectors = []\n",
    "# for i in range(12):\n",
    "#     latent_vectors.append(np.random.normal(size=(128,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Piano.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>zero_crossings</th>\n",
       "      <th>spectrogram</th>\n",
       "      <th>mel_spectrogram</th>\n",
       "      <th>harmonics</th>\n",
       "      <th>perceptual_shock_wave</th>\n",
       "      <th>spectral_centroids</th>\n",
       "      <th>spectral_centroids_delta</th>\n",
       "      <th>spectral_centroids_accelerate</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc_accelerate_35</th>\n",
       "      <th>mfcc36</th>\n",
       "      <th>mfcc_delta_36</th>\n",
       "      <th>mfcc_accelerate_36</th>\n",
       "      <th>mfcc37</th>\n",
       "      <th>mfcc_delta_37</th>\n",
       "      <th>mfcc_accelerate_37</th>\n",
       "      <th>mfcc38</th>\n",
       "      <th>mfcc_delta_38</th>\n",
       "      <th>mfcc_accelerate_38</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Piano.wav</td>\n",
       "      <td>0.094425</td>\n",
       "      <td>26223</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>2.600821e-08</td>\n",
       "      <td>-3.439959e-08</td>\n",
       "      <td>1168.100368</td>\n",
       "      <td>0.451554</td>\n",
       "      <td>0.035034</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004273</td>\n",
       "      <td>-12.944706</td>\n",
       "      <td>-0.00139</td>\n",
       "      <td>0.00794</td>\n",
       "      <td>-7.300853</td>\n",
       "      <td>-0.029756</td>\n",
       "      <td>0.00713</td>\n",
       "      <td>-1.987275</td>\n",
       "      <td>-0.001077</td>\n",
       "      <td>0.009709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 145 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name  zero_crossing_rate  zero_crossings  spectrogram  \\\n",
       "0  Piano.wav            0.094425           26223        -80.0   \n",
       "\n",
       "   mel_spectrogram     harmonics  perceptual_shock_wave  spectral_centroids  \\\n",
       "0            -80.0  2.600821e-08          -3.439959e-08         1168.100368   \n",
       "\n",
       "   spectral_centroids_delta  spectral_centroids_accelerate  ...  \\\n",
       "0                  0.451554                       0.035034  ...   \n",
       "\n",
       "   mfcc_accelerate_35     mfcc36  mfcc_delta_36  mfcc_accelerate_36    mfcc37  \\\n",
       "0           -0.004273 -12.944706       -0.00139             0.00794 -7.300853   \n",
       "\n",
       "   mfcc_delta_37  mfcc_accelerate_37    mfcc38  mfcc_delta_38  \\\n",
       "0      -0.029756             0.00713 -1.987275      -0.001077   \n",
       "\n",
       "   mfcc_accelerate_38  \n",
       "0            0.009709  \n",
       "\n",
       "[1 rows x 145 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_feature_means('Piano.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment\n",
    "import pandas as pd\n",
    "\n",
    "def extract_feature_means(audio_file_path: str) -> pd.DataFrame:\n",
    "    # config settings\n",
    "    number_of_mfcc = 39 #c.NUMBER_OF_MFCC\n",
    "\n",
    "    # 1. Importing 1 file\n",
    "    y, sr = librosa.load('Piano.wav')\n",
    "\n",
    "    # Trim leading and trailing silence from an audio y (silence before and after the actual audio)\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "\n",
    "    # 2. Fourier Transform\n",
    "    # Default FFT window size\n",
    "    n_fft = 2048   #c.N_FFT  # FFT window size\n",
    "    hop_length = 512 #c.HOP_LENGTH  # number audio of frames between STFT columns (looks like a good default)\n",
    "\n",
    "    # Short-time Fourier transform (STFT)\n",
    "    d_audio = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))\n",
    "\n",
    "    # 3. Spectrogram\n",
    "    # Convert an amplitude spectrogram to Decibels-scaled spectrogram.\n",
    "    db_audio = librosa.amplitude_to_db(d_audio, ref=np.max)\n",
    "\n",
    "    # 4. Create the Mel Spectrograms\n",
    "    s_audio = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    s_db_audio = librosa.amplitude_to_db(s_audio, ref=np.max)\n",
    "\n",
    "    # 5 Zero crossings\n",
    "\n",
    "    # #6. Harmonics and Perceptrual\n",
    "    # Note:\n",
    "    #\n",
    "    # Harmonics are characteristichs that represent the sound color\n",
    "    # Perceptrual shock wave represents the sound rhythm and emotion\n",
    "    y_harm, y_perc = librosa.effects.hpss(y)\n",
    "\n",
    "    # 7. Spectral Centroid\n",
    "    # Note: Indicates where the ”centre of mass” for a sound is located and is calculated\n",
    "    # as the weighted mean of the frequencies present in the sound.\n",
    "\n",
    "    # Calculate the Spectral Centroids\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    spectral_centroids_delta = librosa.feature.delta(spectral_centroids)\n",
    "    spectral_centroids_accelerate = librosa.feature.delta(spectral_centroids, order=2)\n",
    "\n",
    "    # spectral_centroid_feats = np.stack((spectral_centroids, delta, accelerate))  # (3, 64, xx)\n",
    "\n",
    "    # 8. Chroma Frequencies¶\n",
    "    # Note: Chroma features are an interesting and powerful representation\n",
    "    # for music audio in which the entire spectrum is projected onto 12 bins\n",
    "    # representing the 12 distinct semitones ( or chromas) of the musical octave.\n",
    "\n",
    "    # Increase or decrease hop_length to change how granular you want your data to be\n",
    "    hop_length = 512 # c.HOP_LENGTH\n",
    "\n",
    "    # Chromogram\n",
    "    chromagram = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=hop_length)\n",
    "\n",
    "    # 9. Tempo BPM (beats per minute)¶\n",
    "    # Note: Dynamic programming beat tracker.\n",
    "\n",
    "    # Create Tempo BPM variable\n",
    "    tempo_y, _ = librosa.beat.beat_track(y=y, sr=sr)\n",
    "\n",
    "    # 10. Spectral Rolloff\n",
    "    # Note: Is a measure of the shape of the y. It represents the frequency below which a specified\n",
    "    #  percentage of the total spectral energy(e.g. 85 %) lies.\n",
    "\n",
    "    # Spectral RollOff Vector\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "\n",
    "    # spectral flux\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "\n",
    "    # Spectral Bandwidth¶\n",
    "    # The spectral bandwidth is defined as the width of the band of light at one-half the peak\n",
    "    # maximum (or full width at half maximum [FWHM]) and is represented by the two vertical\n",
    "    # red lines and λSB on the wavelength axis.\n",
    "    spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "    spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(y=y, sr=sr, p=3)[0]\n",
    "    spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(y=y, sr=sr, p=4)[0]\n",
    "\n",
    "    audio_features = {\n",
    "        \"file_name\": audio_file_path,\n",
    "        \"zero_crossing_rate\": np.mean(librosa.feature.zero_crossing_rate(y)[0]),\n",
    "        \"zero_crossings\": np.sum(librosa.zero_crossings(y, pad=False)),\n",
    "        \"spectrogram\": np.mean(db_audio[0]),\n",
    "        \"mel_spectrogram\": np.mean(s_db_audio[0]),\n",
    "        \"harmonics\": np.mean(y_harm),\n",
    "        \"perceptual_shock_wave\": np.mean(y_perc),\n",
    "        \"spectral_centroids\": np.mean(spectral_centroids),\n",
    "        \"spectral_centroids_delta\": np.mean(spectral_centroids_delta),\n",
    "        \"spectral_centroids_accelerate\": np.mean(spectral_centroids_accelerate),\n",
    "        \"chroma1\": np.mean(chromagram[0]),\n",
    "        \"chroma2\": np.mean(chromagram[1]),\n",
    "        \"chroma3\": np.mean(chromagram[2]),\n",
    "        \"chroma4\": np.mean(chromagram[3]),\n",
    "        \"chroma5\": np.mean(chromagram[4]),\n",
    "        \"chroma6\": np.mean(chromagram[5]),\n",
    "        \"chroma7\": np.mean(chromagram[6]),\n",
    "        \"chroma8\": np.mean(chromagram[7]),\n",
    "        \"chroma9\": np.mean(chromagram[8]),\n",
    "        \"chroma10\": np.mean(chromagram[9]),\n",
    "        \"chroma11\": np.mean(chromagram[10]),\n",
    "        \"chroma12\": np.mean(chromagram[11]),\n",
    "        \"tempo_bpm\": tempo_y,\n",
    "        \"spectral_rolloff\": np.mean(spectral_rolloff),\n",
    "        \"spectral_flux\": np.mean(onset_env),\n",
    "        \"spectral_bandwidth_2\": np.mean(spectral_bandwidth_2),\n",
    "        \"spectral_bandwidth_3\": np.mean(spectral_bandwidth_3),\n",
    "        \"spectral_bandwidth_4\": np.mean(spectral_bandwidth_4),\n",
    "    }\n",
    "\n",
    "    # extract mfcc feature\n",
    "    mfcc_df = extract_mfcc_feature_means(audio_file_path,\n",
    "                                    y,\n",
    "                                    sample_rate=sr,\n",
    "                                    number_of_mfcc=number_of_mfcc)\n",
    "\n",
    "    df = pd.DataFrame.from_records(data=[audio_features])\n",
    "\n",
    "    df = pd.merge(df, mfcc_df, on='file_name')\n",
    "\n",
    "    return df\n",
    "\n",
    "    # librosa.feature.mfcc(y)[0, 0]\n",
    "\n",
    "def extract_mfcc_feature_means(audio_file_name: str,\n",
    "                          y: np.ndarray,\n",
    "                          sample_rate: int,\n",
    "                          number_of_mfcc: int) -> pd.DataFrame:\n",
    "\n",
    "    mfcc_alt = librosa.feature.mfcc(y=y, sr=sample_rate,\n",
    "                                    n_mfcc=number_of_mfcc)\n",
    "    delta = librosa.feature.delta(mfcc_alt)\n",
    "    accelerate = librosa.feature.delta(mfcc_alt, order=2)\n",
    "\n",
    "    mfcc_features = {\n",
    "        \"file_name\": audio_file_name,\n",
    "    }\n",
    "\n",
    "    for i in range(0, number_of_mfcc):\n",
    "        # dict.update({'key3': 'geeks'})\n",
    "\n",
    "        # mfcc coefficient\n",
    "        key_name = \"\".join(['mfcc', str(i)])\n",
    "        mfcc_value = np.mean(mfcc_alt[i])\n",
    "        mfcc_features.update({key_name: mfcc_value})\n",
    "\n",
    "        # mfcc delta coefficient\n",
    "        key_name = \"\".join(['mfcc_delta_', str(i)])\n",
    "        mfcc_value = np.mean(delta[i])\n",
    "        mfcc_features.update({key_name: mfcc_value})\n",
    "\n",
    "        # mfcc accelerate coefficient\n",
    "        key_name = \"\".join(['mfcc_accelerate_', str(i)])\n",
    "        mfcc_value = np.mean(accelerate[i])\n",
    "        mfcc_features.update({key_name: mfcc_value})\n",
    "\n",
    "    df = pd.DataFrame.from_records(data=[mfcc_features])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y, sr = librosa.load('Piano.wav')\n",
    "# # FROM THE INITIAL REPO WE LOOKED AT\n",
    "\n",
    "# def raw_chroma(y, sr, type=\"cens\", nearest_neighbor=True):\n",
    "#     \"\"\"Creates chromagram\n",
    "\n",
    "#     Args:\n",
    "#         audio (np.array): Audio signal\n",
    "#         sr (int): Sampling rate of the audio\n",
    "#         type (str, optional): [\"cens\", \"cqt\", \"stft\", \"deep\", \"clp\"]. Which strategy to use to calculate the chromagram. Defaults to \"cens\".\n",
    "#         nearest_neighbor (bool, optional): Whether to post process using nearest neighbor smoothing. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         np.array, shape=(12, n_frames): Chromagram\n",
    "#     \"\"\"\n",
    "#     if type == \"cens\":\n",
    "#         ch = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "#     elif type == \"cqt\":\n",
    "#         ch = librosa.feature.chroma_cqt(y=y, sr=sr)\n",
    "#     elif type == \"stft\":\n",
    "#         ch = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "#     elif type == \"deep\":\n",
    "#         sig = mm.audio.signal.Signal(y, num_channels=1, sample_rate=sr)\n",
    "#         ch = mm.audio.chroma.DeepChromaProcessor().process(sig).T\n",
    "#     elif type == \"clp\":\n",
    "#         sig = mm.audio.signal.Signal(y, num_channels=1, sample_rate=sr)\n",
    "#         ch = mm.audio.chroma.CLPChromaProcessor().process(sig).T\n",
    "#     else:\n",
    "#         print(\"chroma type not recognized, options are: [cens, cqt, deep, clp, or stft]. defaulting to cens...\")\n",
    "#         ch = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "\n",
    "#     if nearest_neighbor:\n",
    "#         ch = np.minimum(ch, librosa.decompose.nn_filter(ch, aggregate=np.median, metric=\"cosine\"))\n",
    "\n",
    "#     return ch\n",
    "\n",
    "# extract_feature_means(audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.6677261e-03, 8.6677261e-03, 8.6677261e-03, ..., 4.1059726e-03,\n",
       "        3.3435205e-03, 2.5855368e-03],\n",
       "       [3.7310336e-04, 3.7310336e-04, 3.7310336e-04, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [4.5992307e-02, 4.5992307e-02, 4.5918189e-02, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [6.6661917e-02, 6.4879470e-02, 6.3907728e-02, ..., 9.3177566e-03,\n",
       "        7.7836667e-03, 6.2322416e-03],\n",
       "       [7.0036942e-01, 7.0175600e-01, 7.0278281e-01, ..., 4.4614017e-01,\n",
       "        4.4060639e-01, 4.3500450e-01],\n",
       "       [1.9278200e-01, 1.8861033e-01, 1.8548153e-01, ..., 1.9158411e-01,\n",
       "        1.9241178e-01, 1.9317850e-01]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_chroma(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 12), dtype=float32, numpy=\n",
       "array([[ 0.0000000e+00,  0.0000000e+00,  1.2280074e-06, ...,\n",
       "         4.5753539e-02,  5.0049865e-01,  4.3315566e-01],\n",
       "       [ 0.0000000e+00,  0.0000000e+00, -1.2862054e-06, ...,\n",
       "         4.6568520e-02,  5.0693250e-01,  4.3466985e-01],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  1.3451433e-06, ...,\n",
       "         4.4929486e-02,  5.0334269e-01,  4.3654901e-01],\n",
       "       ...,\n",
       "       [ 0.0000000e+00,  0.0000000e+00, -1.0115718e-06, ...,\n",
       "         1.6192013e-02,  3.0769846e-01,  4.2789036e-01],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  1.0683116e-06, ...,\n",
       "         1.6289216e-02,  3.0756676e-01,  4.3086118e-01],\n",
       "       [ 0.0000000e+00,  0.0000000e+00, -1.1221838e-06, ...,\n",
       "         1.6114278e-02,  3.0039692e-01,  4.3377307e-01]], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# import scipy.signal as signal\n",
    "\n",
    "# def chroma(audio, sr, n_frames, margin=16, type=\"cens\", notes=12):\n",
    "#     \"\"\"Creates chromagram for the harmonic component of the audio\n",
    "\n",
    "#     Args:\n",
    "#         audio (np.array): Audio signal\n",
    "#         sr (int): Sampling rate of the audio\n",
    "#         n_frames (int): Total number of frames to resample envelope to\n",
    "#         margin (int, optional): For harmonic source separation, higher values create more extreme separations. Defaults to 16.\n",
    "#         type (str, optional): [\"cens\", \"cqt\", \"stft\", \"deep\", \"clp\"]. Which strategy to use to calculate the chromagram. Defaults to \"cens\".\n",
    "#         notes (int, optional): Number of notes to use in output chromagram (e.g. 5 for pentatonic scale, 7 for standard western scales). Defaults to 12.\n",
    "\n",
    "#     Returns:\n",
    "#         th.tensor, shape=(n_frames, 12): Chromagram\n",
    "#     \"\"\"\n",
    "#     y_harm = librosa.effects.harmonic(y=audio, margin=margin)\n",
    "#     chroma = raw_chroma(y_harm, sr, type=type).T\n",
    "#     chroma = signal.resample(chroma, n_frames)\n",
    "#     notes_indices = np.argsort(np.median(chroma, axis=0))[:notes]\n",
    "#     chroma = chroma[:, notes_indices]\n",
    "#     chroma = tf.convert_to_tensor(chroma / chroma.sum(1)[:, None])#.float()\n",
    "#     return chroma\n",
    "\n",
    "# chroma(y, sr, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import audioreactive as ar\n",
    "\n",
    "# def chroma_weight_latents(chroma, latents):\n",
    "#     \"\"\"Creates chromagram weighted latent sequence\n",
    "\n",
    "#     Args:\n",
    "#         chroma (th.tensor): Chromagram\n",
    "#         latents (th.tensor): Latents (must have same number as number of notes in chromagram)\n",
    "\n",
    "#     Returns:\n",
    "#         th.tensor: Chromagram weighted latent sequence\n",
    "#     \"\"\"\n",
    "#     base_latents = (chroma[..., None, None] * latents[None, ...]).sum(1)\n",
    "#     return base_latents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
