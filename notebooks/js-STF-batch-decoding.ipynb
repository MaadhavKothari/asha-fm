{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Start to Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import librosa, librosa.display\n",
    "import numpy as np\n",
    "import random \n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from spleeter.separator import Separator\n",
    "\n",
    "import moviepy.editor as mp\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "\n",
    "start = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=10000, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DRUM_AUDIO_PATH = '../notebooks/raw_data/audio/MDU.mp3'\n",
    "audio_path = '../notebooks/raw_data/audio/MDU.mp3'\n",
    "AUDIO_FILE_NAME = '13'\n",
    "RAW_AUDIO_PATH = '../notebooks/raw_data/audio/MDU.mp3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# IO\n",
    "# AUDIO_FILE_NAME = RAW_AUDIO_PATH.split('/')[-1].split('.')[0]\n",
    "\n",
    "# SPLIT_AUDIO_PATH = f\"../raw_data/audio/{AUDIO_FILE_NAME}/\"\n",
    "# DRUM_AUDIO_PATH = f\"{SPLIT_AUDIO_PATH}{AUDIO_FILE_NAME}-drums.wav\"\n",
    "\n",
    "VIDEO_SAVE_NAME = f\"{AUDIO_FILE_NAME}-asha.mp4\"\n",
    "VIDEO_SAVE_PATH = f\"../outputs/{VIDEO_SAVE_NAME}\"\n",
    "\n",
    "# GLOBAL\n",
    "# FIXED DURATION\n",
    "\n",
    "# AUDIO \n",
    "FRAME_RATE = 60\n",
    "HOP_WINDOW = 512\n",
    "\n",
    "# Images\n",
    "IMAGE_TIME_GAP = 1 # seconds, play with this\n",
    "IMAGE_FRAME_GAP = IMAGE_TIME_GAP * FRAME_RATE\n",
    "\n",
    "# VIDEO\n",
    "MAX_VIDEO_DURATION = 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the Audio into stems\n",
    "\n",
    "With spleeter by deezer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Audio info from drum stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_params(audio_path):\n",
    "    # Load song with 22050 sample rate\n",
    "    y, sr = librosa.load(audio_path, sr=22050)\n",
    "    audio_duration = librosa.get_duration(y, sr=sr)\n",
    "    \n",
    "    total_frames_float = audio_duration * FRAME_RATE\n",
    "    total_frames = math.ceil(total_frames_float)\n",
    "    \n",
    "    sample_rate = round(total_frames_float / audio_duration * HOP_WINDOW)\n",
    "    \n",
    "    return total_frames, sample_rate, audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onset_info(audio_path, sample_rate):\n",
    "    # Load\n",
    "    y, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    \n",
    "    # Onset strengths and normalize\n",
    "    onset_strengths = librosa.onset.onset_strength(y=y, sr=sample_rate, aggregate=np.median)\n",
    "    onset_strengths = librosa.util.normalize(onset_strengths)\n",
    "    \n",
    "    # Onset timestamps and frames\n",
    "    onset_times = librosa.times_like(onset_strengths, sr=sample_rate)\n",
    "    onset_frames = onset_times * FRAME_RATE\n",
    "    \n",
    "    onset_info = np.concatenate([\n",
    "        onset_frames.reshape((-1, 1)),\n",
    "        onset_times.reshape((-1, 1)),\n",
    "        onset_strengths.reshape((-1, 1))\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Beat times\n",
    "    beat_times = librosa.beat.beat_track(y=y, sr=sample_rate, units='time')[1]\n",
    "    \n",
    "    return y, onset_info, beat_times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(602, 30720, 10.033015873015874)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_frames, sample_rate, audio_duration = get_audio_params(DRUM_AUDIO_PATH)\n",
    "total_frames, sample_rate, audio_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((308214,), (602, 3), (19,))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, onset_info, beat_times = get_onset_info(DRUM_AUDIO_PATH, sample_rate)\n",
    "y.shape, onset_info.shape, beat_times.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create additional onset strength features\n",
    "\n",
    "- Linear Decay\n",
    "- Exponential Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_onset_features(onset_info, beat_times, lin_decay_time=0.25, exp_decay_rate=0.25, decay_magnification=True):\n",
    "    \n",
    "    fixed_decay_frames = int(lin_decay_time * FRAME_RATE)\n",
    "    \n",
    "    # Create column of zeroes as default value for both Linear and Exp Decay\n",
    "    onset_info = np.concatenate([onset_info, np.zeros((onset_info.shape[0], 1)), np.zeros((onset_info.shape[0], 1))], axis=1)\n",
    "    \n",
    "    # for each row\n",
    "    for i in range(onset_info.shape[0]):\n",
    "        # Skip the first value, makes life easy\n",
    "        if i == 0:\n",
    "            onset_info[i, 3] = 0\n",
    "        \n",
    "        \n",
    "        ## LINEAR\n",
    "        # If the timestamp is in beat_times, it's a peak\n",
    "        if onset_info[i, 1] in beat_times:\n",
    "            onset_info[i, 3] = onset_info[i, 2] # Linear Column\n",
    "            \n",
    "            # Decay Params\n",
    "            decay_factor = 1\n",
    "            if decay_magnification:\n",
    "                decay_factor *= (onset_info[i, 2] + 1)\n",
    "            \n",
    "            decay_frames = fixed_decay_frames * decay_factor\n",
    "            lin_decay_val = onset_info[i, 2] / decay_frames\n",
    "        \n",
    "        # Check if the previous value is zero or less than the decay_val -> 0\n",
    "        if onset_info[i - 1, 3] == 0. or abs(onset_info[i -1, 3]) < lin_decay_val:\n",
    "            pass\n",
    "        \n",
    "        # If previous value > 0, needs decay\n",
    "        elif onset_info[i - 1, 3] > 0: \n",
    "            onset_info[i, 3] = onset_info[i - 1, 3] - lin_decay_val\n",
    "        \n",
    "        \n",
    "        # EXPONENTIAL\n",
    "        # If the timestamp is in beat_times, it's a peak\n",
    "        if onset_info[i, 1] in beat_times:\n",
    "            onset_info[i, 4] = onset_info[i, 2] # Exp Column\n",
    "        \n",
    "        # Set current to zero if previous is zero or small number\n",
    "        elif onset_info[i - 1, 4] == 0 or onset_info[i - 1, 4] < 0.005: \n",
    "            pass\n",
    "        \n",
    "        # If previous value > 0, needs decay\n",
    "        elif onset_info[i - 1, 4] != 0: \n",
    "                onset_info[i, 4] = onset_info[i - 1, 4] * (1 - exp_decay_rate)\n",
    "    \n",
    "    return onset_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, onset_info, beat_times = get_onset_info(DRUM_AUDIO_PATH, sample_rate)\n",
    "onset_info = create_onset_features(onset_info, beat_times, lin_decay_time=0.25, exp_decay_rate=0.25, decay_magnification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Reshape, Conv2D, Conv2DTranspose, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ds = image_dataset_from_directory('../raw_data/zelle', label_mode=None, image_size=(448, 448), batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = ds.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgs = ds.as_numpy_iterator().__next__()\n",
    "# imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Model Architecture\n",
    "\n",
    "Because we are using load weights, we need to instantiate the model every time. This sucks for notebooks, but it will be fine once everything is packaged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.random.normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 200\n",
    "\n",
    "## Encoder\n",
    "\n",
    "input_image = Input(shape=(448, 448, 3))\n",
    "    \n",
    "x = Conv2D(32, (3, 3), padding='same', activation=\"relu\")(input_image)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(128, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Conv2D(256, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "# Encoder Build\n",
    "encoder = Model(input_image, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "#############################################\n",
    "\n",
    "## Decoder\n",
    "\n",
    "latent_inputs = Input(shape=(latent_dim,))  \n",
    "y = Dense(7*7*128, activation='tanh')(latent_inputs)\n",
    "y = Reshape((7, 7, 128))(y)\n",
    "\n",
    "y = Conv2DTranspose(256, (3, 3), strides=2, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(256, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(256, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(256, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "\n",
    "y = Conv2DTranspose(128, (3, 3), strides=2, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(128, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "\n",
    "y = Conv2DTranspose(64, (3, 3), strides=2, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(64, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(64, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "\n",
    "y = Conv2DTranspose(32, (3, 3), strides=2, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(32, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(32, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "\n",
    "y = Conv2DTranspose(16, (3, 3), strides=2, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(16, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "y = Conv2DTranspose(16, (3, 3), strides=1, padding='same', activation=\"relu\")(y)\n",
    "\n",
    "decoder_output = Conv2DTranspose(3, (3, 3), strides=2, padding='same', activation='sigmoid')(y)\n",
    "\n",
    "# Decoder Build\n",
    "decoder = Model(inputs=latent_inputs, outputs=decoder_output, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(tf.keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = tf.keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    tf.keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model and Load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f7793402e30>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "\n",
    "vae.load_weights('vae_complex_model_epoch5100')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "Loads encoded_images numpy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldername = '../raw_data/zelle'\n",
    "\n",
    "# def load_images_from_folder(foldername, output_folder):\n",
    "#     latent_space = []\n",
    "#     for filename in os.listdir(foldername):\n",
    "#         with Image.open(os.path.join(foldername,filename)).resize((448,448)) as img:\n",
    "#             if img is not None:\n",
    "#                 img = np.asarray(img)\n",
    "#                 encoded = vae.encoder(img.reshape(-1,448,448,3)/255)\n",
    "#                 latent_space.append(encoded[2])\n",
    "#     latent_space = np.concatenate(latent_space, axis=0)\n",
    "#     np.save('encoded_images.npy', latent_space)\n",
    "#     return latent_space\n",
    "\n",
    "# load_images_from_folder(foldername, foldername)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"encoded_images_npy\"\n",
    "\n",
    "def load_images_from_file(filename):\n",
    "    latent_space = np.load(filename)\n",
    "    return latent_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_IMAGES = math.ceil(audio_duration / IMAGE_TIME_GAP + 1)\n",
    "N_IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_images = load_images_from_file(\"encoded_images.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_images = vae.decoder.predict(encoded_images[0:3 + N_IMAGES])[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2355, 200)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2187"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_start_point = np.random.randint(0, encoded_images.shape[0] - N_IMAGES, size=(1))[0]\n",
    "random_start_point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Interpolated Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interp_vecs(N_IMAGES, encoded_images, min_factor=0.01, onset_info=onset_info):\n",
    "    # Empty lists to append new vectors to\n",
    "    lin_interp_vecs = []\n",
    "    exp_interp_vecs = []\n",
    "    \n",
    "    # We want to interate through the images to be able to interpolate between them\n",
    "    for i in range(N_IMAGES - 1):\n",
    "        \n",
    "        # Get start and stop latent vectors\n",
    "        start_vec = encoded_images[i]\n",
    "        end_vec = encoded_images[i + 1]\n",
    "        \n",
    "        # get step_vec between image_n and image_n + 1\n",
    "        step_vec = (end_vec - start_vec) / IMAGE_FRAME_GAP\n",
    "            \n",
    "        # We now need to create an image at every frame \n",
    "        # Mad props to Charlotte for making this way simpler than I was trying to make it\n",
    "        for j in range(IMAGE_FRAME_GAP):\n",
    "            \n",
    "            current_frame = IMAGE_FRAME_GAP * i + j\n",
    "            # Check to see if current_frame > total_frames : no point making extra images\n",
    "            if current_frame > int(onset_info[-1,0]):\n",
    "                break\n",
    "            lin_interp_vecs.append(start_vec + (step_vec * j * (onset_info[current_frame, 3] / min_factor + 1)))\n",
    "            exp_interp_vecs.append(start_vec + (step_vec * j * (onset_info[current_frame, 4] / min_factor + 1)))\n",
    "            \n",
    "    return np.array(lin_interp_vecs), np.array(exp_interp_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_interp_vecs, exp_interp_vecs = create_interp_vecs(N_IMAGES, encoded_images, min_factor=1, onset_info=onset_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((602, 200), (602, 200))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_interp_vecs.shape, exp_interp_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_interp_imgs(decoder, lin_interp_vecs=lin_interp_vecs, exp_interp_vecs=exp_interp_vecs):\n",
    "#     # Linear Images\n",
    "#     lin_interp_imgs = decoder.predict(lin_interp_vecs)\n",
    "#     lin_interp_imgs = (lin_interp_imgs * 255).astype('uint8')\n",
    "    \n",
    "#     # Exponential Images\n",
    "#     exp_interp_imgs = decoder.predict(exp_interp_vecs)\n",
    "#     exp_interp_imgs = (exp_interp_imgs * 255).astype('uint8')\n",
    "    \n",
    "#     return lin_interp_imgs, exp_interp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import os\n",
    "\n",
    "save_path = '../notebooks/images'\n",
    "\n",
    "\n",
    "def create_interp_imgs(decoder, lin_interp_vecs, exp_interp_vecs, batch_size=1, save_path=save_path):\n",
    "    # Linear Images\n",
    "    batch_memory = 500 * 1024 * 1024  # 500 MB\n",
    "\n",
    "    num_lin_imgs = len(lin_interp_vecs)\n",
    "    lin_interp_imgs = []\n",
    "    for i in range(0, num_lin_imgs, batch_size):\n",
    "        batch_vecs = lin_interp_vecs[i:i+batch_size]\n",
    "        batch_imgs = decoder.predict(batch_vecs)\n",
    "        batch_imgs = (batch_imgs * 255).astype('uint8')\n",
    "        lin_interp_imgs.append(batch_imgs)\n",
    "        print(f\"Processed {i+batch_size} linear images\")\n",
    "        if save_path and i % (500 * 1024 * 1024 // batch_memory) == 0:\n",
    "            if len(lin_interp_imgs) > 0:\n",
    "                np.save(os.path.join(save_path, f\"lin_interp_imgs_{i}.npy\"), np.concatenate(lin_interp_imgs, axis=0))\n",
    "                lin_interp_imgs = []\n",
    "        K.clear_session()\n",
    "    if len(lin_interp_imgs) > 0:\n",
    "        lin_interp_imgs = np.concatenate(lin_interp_imgs, axis=0)\n",
    "        if save_path:\n",
    "            np.save(os.path.join(save_path, f\"lin_interp_imgs_{num_lin_imgs}.npy\"), lin_interp_imgs)\n",
    "    \n",
    "    # Exponential Images\n",
    "    num_exp_imgs = len(exp_interp_vecs)\n",
    "    exp_interp_imgs = []\n",
    "    for i in range(0, num_exp_imgs, batch_size):\n",
    "        batch_vecs = exp_interp_vecs[i:i+batch_size]\n",
    "        batch_imgs = decoder.predict(batch_vecs)\n",
    "        batch_imgs = (batch_imgs * 255).astype('uint8')\n",
    "        exp_interp_imgs.append(batch_imgs)\n",
    "        print(f\"Processed {i+batch_size} exponential images\")\n",
    "        if save_path and i % (500 * 1024 * 1024 // batch_memory) == 0:\n",
    "            if len(exp_interp_imgs) > 0:\n",
    "                np.save(os.path.join(save_path, f\"exp_interp_imgs_{i}.npy\"), np.concatenate(exp_interp_imgs, axis=0))\n",
    "                exp_interp_imgs = []\n",
    "        K.clear_session()\n",
    "    if len(exp_interp_imgs) > 0:\n",
    "        exp_interp_imgs = np.concatenate(exp_interp_imgs, axis=0)\n",
    "        if save_path:\n",
    "            np.save(os.path.join(save_path, f\"exp_interp_imgs_{num_exp_imgs}.npy\"), exp_interp_imgs)\n",
    "    \n",
    "    return lin_interp_imgs, exp_interp_imgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "lin_imgs, exp_imgs = create_interp_imgs(vae.decoder, lin_interp_vecs=lin_interp_vecs, exp_interp_vecs=exp_interp_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_lin_interp_imgs(save_path, num_lin_imgs):\n",
    "    lin_interp_imgs = []\n",
    "    for i in range(num_lin_imgs):\n",
    "        file_path = os.path.join(save_path, f\"lin_interp_imgs_{i}.npy\")\n",
    "        if os.path.exists(file_path):\n",
    "            lin_interp_imgs.append(np.load(file_path))\n",
    "    lin_interp_imgs = np.concatenate(lin_interp_imgs, axis=0)\n",
    "    return lin_interp_imgs\n",
    "\n",
    "def concatenate_export_interp_imgs(save_path, num_exp_imgs):\n",
    "    exp_interp_imgs = []\n",
    "    for i in range(num_exp_imgs):\n",
    "        file_path = os.path.join(save_path, f\"exp_interp_imgs_{i}.npy\")\n",
    "        if os.path.exists(file_path):\n",
    "            exp_interp_imgs.append(np.load(file_path))\n",
    "    exp_interp_imgs = np.concatenate(exp_interp_imgs, axis=0)\n",
    "    return exp_interp_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lin_imgs = len(lin_interp_vecs)\n",
    "num_exp_imgs = len(exp_interp_vecs)\n",
    "\n",
    "interp_imgs = concatenate_lin_interp_imgs(save_path, num_lin_imgs)\n",
    "exp_imgs = concatenate_lin_interp_imgs(save_path, num_exp_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_video_with_audio(interp_imgs):\n",
    "    \n",
    "    # Load the audio file\n",
    "    audio_clip = mp.AudioFileClip(RAW_AUDIO_PATH)\n",
    "    \n",
    "    # Needed for some reason?\n",
    "    frames = [mp.ImageClip(interp_img).set_duration(1/FRAME_RATE) for interp_img in interp_imgs]\n",
    "    \n",
    "    # Combine the frames into a video clip\n",
    "    video_clip = mp.concatenate_videoclips(frames, method='chain')\n",
    "    \n",
    "    # Overlay the audio on the video clip\n",
    "    final_clip = video_clip.set_audio(audio_clip)\n",
    "    \n",
    "    # Write the final video clip to file\n",
    "    final_clip.write_videofile(\"donk.mp4\", fps=FRAME_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../outputs/13-asha.mp4'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_SAVE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video donk.mp4.\n",
      "MoviePy - Writing audio in donkTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video donk.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready donk.mp4\n"
     ]
    }
   ],
   "source": [
    "create_video_with_audio(interp_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:30.719698\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
